#!/usr/bin/env python3
from sea_ice_rs.install import install

install()

import argparse
import os
import sys
import cv2
import csv
import random
from multiprocessing import Process, Manager
from tqdm import tqdm
from sea_ice_rs.utils import decompose_filepath


tr_dataset = Manager().list()
te_dataset = Manager().list()


def sampling_probability(dist_stats_file):
    """
    The function sets the probabilitiy of a sample to be included in the dataset.
    A sample in the class with a higher occurrence is less likely to be selected.
    The class with the smallest occurrence will have all of its samples included.
    The expected number of samples per class is equal throughout the entire data.
    """
    with open(dist_stats_file, "r") as dist_file:
        dist_file_reader = csv.reader(dist_file)
        dist_stats = [row for row in dist_file_reader]
        labels = dist_stats[0]
        counts = list(map(float, dist_stats[1]))

        null_idx = labels.index("")
        del labels[null_idx]
        del counts[null_idx]

        return {int(labels[i]): min(counts) / counts[i] for i in range(len(labels))}


def patch_location_map(patch_loc_file):
    """
    Read (X, Y) coordinates of the patches.
    """
    with open(patch_loc_file, "r") as patch_locs:
        patch_loc_reader = csv.reader(patch_locs)
        return {row[0][1:]: (row[1], row[2]) for row in patch_loc_reader}


def sampling(images, img_dir, mask_dir, prob_dict, patch_loc_dict, thread_num):
    """
    Sample the data using the probabilities defined.
    """
    pbar = tqdm(images)
    for img in pbar:
        pbar.set_description(f"Thread #{thread_num}")
        _, filename, extension = decompose_filepath(img)
        if extension != "jpg":
            return
        patch_num = filename.split("-")[0][1:]
        year = filename.split("-")[1][0:4]
        month = filename.split("-")[1][4:6]
        day = filename.split("-")[1][6:8]
        hour = filename.split("-")[1][8:10]

        inImage = cv2.imread(f"{img_dir}/{img}")
        inMask = cv2.imread(f"{mask_dir}/{filename}-mask.png")

        for row in range(inImage.shape[0]):
            for col in range(inImage.shape[1]):
                label = inMask[row][col][0]
                sampling_weights = [
                    1 - prob_dict[label],
                    prob_dict[label] * 0.8,
                    prob_dict[label] * 0.2,
                ]
                selection = random.choices(["skip", "tr", "te"], sampling_weights, k=1)[
                    0
                ]

                if selection == "skip":
                    continue

                pix_vals = inImage[row][col]
                sample = [
                    patch_num,
                    patch_loc_dict[patch_num][0],
                    patch_loc_dict[patch_num][1],
                    year,
                    month,
                    day,
                    hour,
                    row,
                    col,
                    pix_vals[0],
                    pix_vals[1],
                    pix_vals[2],
                    label,
                ]
                if selection == "tr":
                    tr_dataset.append(sample)
                elif selection == "te":
                    te_dataset.append(sample)

    print(f"Thread #{thread_num} done", file=sys.stdout)


def write_to_datasets(parent_dir, sample_counts, dataset_type):
    """
    Outputs the train/test datasets to CSV files
    """
    with open(
        f"{parent_dir}/pixel_dataset_{dataset_type}.csv", "w", newline=""
    ) as output_file:

        dataset_writer = csv.writer(output_file)

        if dataset_type == "tr":
            dataset = tr_dataset
        elif dataset_type == "te":
            dataset = te_dataset

        for sample in dataset:
            dataset_writer.writerow(sample)
            sample_counts[sample[-1]] += 1


def dataset_stats(sample_counts, stats_filename):
    """
    Outputs the number of samples per class statistics to a CSV file.
    """
    for k in sample_counts.keys():
        print(f"{k}: {sample_counts[k]}", file=sys.stdout)
        print(f"SUM: {sum(sample_counts.values())}", file=sys.stdout)

    with open(stats_filename, "w", newline="") as file:
        writer = csv.writer(file)
        for item in sample_counts.items():
            writer.writerow(item)
        writer.writerow(["SUM", sum(sample_counts.values())])


def main(args):
    prob_dict = sampling_probability(args.dist)
    patch_loc_dict = patch_location_map(args.patch_loc)

    parent_dir = decompose_filepath(decompose_filepath(args.images)[0])[0]

    processes = list()

    # Use multiprocessing to speed up
    num_images = len(os.listdir(args.images))
    thread_size = args.num_threads
    for i in range(0, num_images, thread_size):
        images = os.listdir(args.images)[i : i + thread_size]
        process_num = int(i / thread_size)
        p = Process(
            target=sampling,
            args=(
                images,
                args.images,
                args.masks,
                prob_dict,
                patch_loc_dict,
                process_num,
            ),
        )
        processes.append(p)
        p.start()

    for p in processes:
        p.join()

    train_sample_counts = {k: 0 for k in prob_dict.keys()}
    test_sample_counts = {k: 0 for k in prob_dict.keys()}
    write_to_datasets(parent_dir, train_sample_counts, "tr")
    write_to_datasets(parent_dir, test_sample_counts, "te")

    print("Train Dataset Statistics", file=sys.stdout)
    dataset_stats(train_sample_counts, f"{parent_dir}/pixel_dataset_stats_tr.csv")
    print("\nTest Dataset Statistics", file=sys.stdout)
    dataset_stats(test_sample_counts, f"{parent_dir}/pixel_dataset_stats_te.csv")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--num-threads", type=str, help="Number of threads for the process", default=400
    )
    parser.add_argument("--images", type=str, help="Images directory path")
    parser.add_argument("--masks", type=str, help="Masks directory path")
    parser.add_argument("--dist", type=str, help="Distribution statistics CSV file")
    parser.add_argument(
        "--patch-loc", type=str, help="Patch locations ((X, Y) coordinates) CSV file"
    )

    args = parser.parse_args()
    main(args)
