#!/usr/bin/env python3
from sea_ice_rs.install import install

install()

import argparse
import os
import cv2
import csv
import random
from multiprocessing import Process, Manager
from tqdm import tqdm
from sea_ice_rs.utils import decompose_filepath


dataset = Manager().list()


def sampling_probability(dist_stats_file):
    with open(dist_stats_file, "r") as dist_file:
        dist_file_reader = csv.reader(dist_file)
        dist_stats = [row for row in dist_file_reader]
        labels = dist_stats[0]
        counts = list(map(float, dist_stats[1]))

        null_idx = labels.index("")
        del labels[null_idx]
        del counts[null_idx]

        return {int(labels[i]): min(counts) / counts[i] for i in range(len(labels))}


def patch_location_map(patch_loc_file):
    with open(patch_loc_file, "r") as patch_locs:
        patch_loc_reader = csv.reader(patch_locs)
        return {row[0][1:]: (row[1], row[2]) for row in patch_loc_reader}


def dataset_stats(sample_counts, stats_filename):
    for k in sample_counts.keys():
        print(f"{k}: {sample_counts[k]}", file=stdout)

    with open(stats_filename, "w", newline="") as file:
        writer = csv.writer(file)
        for item in sample_counts.items():
            writer.writerow(item)


def sampling(images, img_dir, mask_dir, prob_dict, patch_loc_dict):
    for img in tqdm(images):
        _, filename, extension = decompose_filepath(img)
        if extension != "jpg":
            return
        patch_num = filename.split("-")[0][1:]
        year = filename.split("-")[1][0:4]
        month = filename.split("-")[1][4:6]
        day = filename.split("-")[1][6:8]
        hour = filename.split("-")[1][8:10]

        inImage = cv2.imread(f"{img_dir}/{img}")
        inMask = cv2.imread(f"{mask_dir}/{filename}-mask.png")

        for row in range(inImage.shape[0]):
            for col in range(inImage.shape[1]):
                label = inMask[row][col][0]
                sampling_weights = [1 - prob_dict[label], prob_dict[label]]
                skip_sampling = random.choices([True, False], sampling_weights, k=1)[0]

                if skip_sampling:
                    continue

                pix_vals = inImage[row][col]
                sample = [
                    patch_num,
                    patch_loc_dict[patch_num][0],
                    patch_loc_dict[patch_num][1],
                    year,
                    month,
                    day,
                    hour,
                    row,
                    col,
                    pix_vals[0],
                    pix_vals[1],
                    pix_vals[2],
                    label,
                ]
                dataset.append(sample)


def write_to_datasets(parent_dir, dataset, train_sample_counts, test_sample_counts):
    train_dataset = open(f"{parent_dir}/pixel_dataset_train.csv", "w", newline="")
    test_dataset = open(f"{parent_dir}/pixel_dataset_test.csv", "w", newline="")
    train_writer = csv.writer(train_dataset)
    test_writer = csv.writer(test_dataset)

    for sample in dataset:
        split_probabilities = [0.8, 0.2]
        train_sample = random.choices([True, False], split_probabilities, k=1)[0]
        if train_sample:
            train_writer.writerow(sample)
            train_sample_counts[sample[-1]] += 1
        else:
            test_writer.writerow(sample)
            test_sample_counts[sample[-1]] += 1

    train_dataset.close()
    test_dataset.close()


def main(args):
    prob_dict = sampling_probability(args.dist)
    patch_loc_dict = patch_location_map(args.patch_loc)

    parent_dir = decompose_filepath(decompose_filepath(args.images)[0])[0]

    processes = list()

    num_images = len(os.listdir(args.images))
    thread_size = 300
    for i in range(0, num_images, thread_size):
        images = os.listdir(args.images)[i : i + thread_size]
        p = Process(
            target=sampling,
            args=(images, args.images, args.masks, prob_dict, patch_loc_dict),
        )
        processes.append(p)
        p.start()

    for p in processes:
        p.join()

    train_sample_counts = {k: 0 for k in prob_dict.keys()}
    test_sample_counts = {k: 0 for k in prob_dict.keys()}
    write_to_datasets(parent_dir, dataset, train_sample_counts, test_sample_counts)

    print("Train Dataset Statistics", file=stdout)
    dataset_stats(train_sample_counts, f"{parent_dir}/pixel_dataset_stats_train.csv")
    print("\nTest Dataset Statistics", file=stdout)
    dataset_stats(test_sample_counts, f"{parent_dir}/pixel_dataset_stats_test.csv")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument("--images", type=str, help="Images directory path")
    parser.add_argument("--masks", type=str, help="Masks directory path")
    parser.add_argument("--dist", type=str, help="Distribution statistics CSV file")
    parser.add_argument(
        "--patch-loc", type=str, help="Patch locations (XY coordinates) CSV file"
    )

    args = parser.parse_args()
    main(args)
