#!/usr/bin/env python3
import argparse
import cv2
import csv
import sys
import numpy as np
import pandas as pd
import yaml
import matplotlib.pyplot as plt
from PIL import Image
import math

from datetime import datetime
from sea_ice_rs.utils import decompose_filepath
from sea_ice_rs.data_processing import (
    patch_location_map,
    GLCM_handler,
    normalize
)
from sea_ice_rs.ML_tools import (
    config_parser,
    process_data,
)
from sklearn.metrics import accuracy_score
from keras.models import load_model


def main(args):
    # Capture image information as 2D array and csv

    headers = [
        "patch_num",
        "year",
        "patch_loc_y",
        "patch_loc_x",
        "DOY",
        "hour",
        "pix_loc_y",
        "pix_loc_x",
        "band_8",
        "band_4",
        "band_3",
    ]

    decomposed = decompose_filepath(args.input)

    img_name = decomposed[1]

    parent_dir = decompose_filepath(decomposed[0])[0]

    dataset_file = f"{parent_dir}/{img_name}.csv"

    dataset = open(dataset_file, "w", newline="")

    csv_writer = csv.writer(dataset)

    csv_writer.writerow(headers)

    inImage = cv2.imread(args.input)

    patch_num = img_name.split("-")[0][1:]

    # Extract date information
    date_info = img_name.split("-")[1]
    year = int(date_info[0:4])
    month = int(date_info[4:6])
    day = int(date_info[6:8])
    hour = int(date_info[8:10])

    doy = int(datetime(year, month, day).strftime("%j"))

    # Obtain XY corrdinates of patches
    patch_loc_dict = patch_location_map(args.patch_loc)
    for row in range(inImage.shape[0]):
            for col in range(inImage.shape[1]):
                pix_vals = inImage[row][col]
                sample = [
                    patch_num,
                    year,
                    patch_loc_dict[patch_num][0],
                    patch_loc_dict[patch_num][1],
                    doy,
                    hour,
                    row,
                    col,
                    pix_vals[0],
                    pix_vals[1],
                    pix_vals[2],
                ]

                csv_writer.writerow(sample)

    # GLCM
    dataset.close()

    GLCM_handler(parent_dir, dataset_file, "jpg", args.input, True)

    glcm_file = f"{parent_dir}/GLCM.csv"

    # Normalize

    df = normalize(glcm_file, args.std_data)

    df['label'] = 0

    col = df.pop("label")

    df.insert(0, col.name, col)

    df.iloc[:math.floor(len(df)/2), 0] = -1

    df.iloc[math.floor(len(df)/2):, 0] = -2

    out = parent_dir + "/" + decomposed[1] + "_norm.csv"

    df.to_csv(out, index=False)

    # Get y_pred as 1D array and convert to image

    _, _, _, _, verbosity, _, kernel_size = config_parser(args.dl_config)

    log_file = open(f"{args.result_dir}/test.log", "w")
    sys.stdout = log_file

    X_te, Y_te, classes = process_data(out, args.dl_config)

    X_te_conv = np.transpose(
        [
            np.transpose(
                np.pad(
                    X_te[0 : X_te.shape[0], X_te.shape[1] - 18 : X_te.shape[1]],
                    ((0, 0), (0, kernel_size)),
                    "wrap",
                )
            )
        ]
    )

    X_te_cat = X_te[0 : X_te.shape[0], 0 : X_te.shape[1] - 15]

    X_te = {"conv": X_te_conv, "cat": X_te_cat}

    checkpoint_path = f"{args.result_dir}/cp.ckpt"
    trained_model = load_model(checkpoint_path)

    y_pred = np.argmax(
        trained_model.predict(x=X_te, batch_size=5, verbose=verbosity), axis=1
    )

    # Show mask image and compare 

    inMask = cv2.imread(f"data/arctic-sea-ice-image-masking/Masks/{img_name}-mask.png")
    plt.imshow(inMask)
    plt.show()

    two_dim_arr = y_pred.reshape(inImage.shape[0], inImage.shape[1])
    plt.imshow(two_dim_arr)
    plt.show()

    # Greyscale 

    plt.imshow(inImage)
    plt.show()


    


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument("--input", type=str, help="Input image")

    parser.add_argument(
        "--patch-loc", type=str, help="Patch locations ((X, Y) coordinates) CSV file"
    )

    parser.add_argument(
        "--std-data", type=str, help="Dataset that provides normalization standard"
    )

    # Call model using load_model and predict
    # Be more specific on the checkpoint, one more level
    # Checkpoint directory instead of result directory
    parser.add_argument(
        "--result-dir",
        type=str,
        help="Directory path with train results including checkpoint files",
    )

    # Config file from DL_config, 
    parser.add_argument(
        "--dl-config",
        type=str,
        help="YAML file containing the configuration for deep learning",
    )

    args = parser.parse_args()
    main(args)
